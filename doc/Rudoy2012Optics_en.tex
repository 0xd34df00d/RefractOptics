\documentclass[11pt,a4paper]{article}
\usepackage{a4wide}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage{graphics,graphicx,epsfig}
\usepackage{amssymb,amsfonts,amsthm,amsmath,mathtext,cite,enumerate,float}
\usepackage[english,russian]{babel}
\usepackage[all]{xy}
\usepackage{morefloats}
\usepackage{pgf}
\usepackage[debug,outputdir={docgraphs/}]{dot2texi}
\usepackage{tikz}
\usepackage{scalefnt}
\usepackage{listings}
\usepackage{float}
\usepackage{verbatim}
\usepackage{placeins}
\usepackage{url}
\usepackage{babelbib}
\usepackage{pbox}
\usepackage{grffile}
\usepackage{color}
\usepackage{xfrac}
\usepackage{comment}
\usepackage{rotating}
\usepackage{cite}
\usepackage{sectsty}
\usepackage{caption}
\usepackage{subcaption}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{decorations.pathmorphing}

% Comment the following block when compiling this .tex with a saner compiler than texlive.
\makeatletter
\def\@settitle{\begin{center}%
    \baselineskip14\p@\relax
    \bfseries
    \@title
  \end{center}%
}

\renewcommand{\citedash}{]-[}
\renewcommand{\citepunct}{], [}

\renewcommand{\thesubfigure}{\asbuk{subfigure}}
\addto\captionsrussian{\renewcommand{\figurename}{Фиг.}}

\sectionfont{\centering\normalfont\small\MakeUppercase}

%\renewcommand{\section}{\@startsection {section}{1}%
%  \z@{.7\linespacing\@plus\linespacing}{.5\linespacing}%
%  {\normalfont}}
%\renewcommand{\section}{\@startsection {section}{1}
%  \z@{2.7ex \@plus 1ex}{1.0ex}%
%  {\normalfont}}
\makeatother

\theoremstyle{definition}
\newtheorem{algo}{Алгоритм}
\newtheorem{theorem}{Теорема}
\newtheorem{stat}{Утверждение}
\newtheorem{defin}{Определение}
\newtheorem{note}{Замечание}

\begin{document}

\begin{center}
  STABILITY OF NON-LINEAR REGRESSION MODELS WITH RESPECT TO VARIATIONS IN THE MEASURED DATA

  \bigskip
  G.~Rudoy.
\end{center}

\begin{abstract}
  A set of inductively generated non-linear regression models is considered to find an optimal
  one. Firstly, a previously suggested model generation method taking the complexity of the
  models into account is applied. Then, a new model selection criteria is proposed, called
  model stability, which shows the dependency of the inaccuracy in determining the coefficients
  of the generated models on the variation of the data in the learning set. This criteria
  is used directly to determine the inaccuracy of the model coefficients, which is of interest
  to the experts, as well as to select the optimal model amongst different ones generated
  using different algorithm hyperparameters.
  The data obtained during experiments on optical dispersion of transparent polymers is used
  to illustrate the method.

  \bigskip
  \textbf{Keywords}: \emph{symbolic regression, non-linear models, inductive generation,
	model stability, transparent polymers dispersion.}
\end{abstract}

\section*{Introduction}

Analysing the results of a physical experiment typically requires finding a functional
dependency between the measured data. It is also very desirable for the dependency to
be interpretable by an expert in the corresponding physical field. In many cases some
theoretical assumptions about the structure of the functional dependency are available,
or a choice should be made between different proposed models.

One of the methods allowing to find interpretable models is symbolic regression
\cite{davidson:2000:snrea,reference/ml/X10vc,StrijovW10,Strijov08InductMethods,Rudoy13},
which generates structurally complex non-linear models. Different models
can be compared by their respective errors on the measured data,
and the optimization of their numeric parameters is performed, for example, using
the Levenberg-Marquardt algorithm\cite{Marquardt1963Algorithm,more:78}.

On the other hand, during physical experiment analysis not only the model
parameters themselves are important for the expert, but the uncertainities in
determining their values resulting from the intrinsic measurement inaccuracies.
For the linear regression this problem is known to have a theoretical solution\cite{Vatunin05}
in the particular case of the independent variable measured exactly and the
dependent variable having the same Gaussian distribution of the error at all
measured points. More complex case of non-linear regression, including the case
of independent variables measured inexactly, as well as all points having
different error distributions, has not been considered as far as we know.

In this paper the non-linear symbolic regression method is applied to find
the dependency of the refraction index $n$ of a polymer as a function of the
wavelength $\lambda$ for those frequencies where the considered polymer is
transparent, including visible and near infrared light. The goal of the
experimenters was to, first, find the dispersion for each polymer, and then
derive the concentration of each polymer in their mixture, given that the
dispersion of the mixture of polymers is a weighted sum of their respective
dispersions. In other words, in case of two polymers, knowing the functions
$n_1(\lambda)$ and $n_2(\lambda)$, the mixture dispersion dependency $n(\lambda)$ 
should be measured and, since
$n(\lambda) = \alpha n_1(\lambda) + (1 - \alpha) n_2(\lambda)$, the concentration
of the first polymer $\alpha$ should be derived.

The refraction indexes for the transparent polymers of a similar chemical
composition differ only slightly. Thus, the uncertainity in determining the
$n(\lambda)$ function coefficients and its dependency on the inaccuracies of measurements
of the wavelength $\lambda$ and refraction index $n$ must be considered.
This dependency is also important because it defines the requirements for the
precision of the devices and, consecutively, it largely
affects the cost and duration of the experiment.

Typically broad spectrum sources are used in refractometers, and the
inaccuracy in extracting a single wavelength is defined by the hardware function
of the monochromator being used and is thorougly considered, for example, in
\cite{Malishev79,Zaidel72}. In most cases the inaccuracy of $\lambda$ can be
computed as well as determined experimentally using narrow light sources like
lasers, known atomic transitions like the mercury triplet or sodium dublet.
Typical relative wavelength measurement inaccuracy is around $0.03 \div 0.5\%$,
thus absolute inaccuracy depends on the wavelength itself. Inaccuracy of
the refraction index $n$ depends on the measurement method and, for example,
in case of using the total internal refraction angle, is defined by the degree
of non-parallelism of the light beams used, the inaccuracy in angle measurement
and so on. The inaccuracy ranges from $(1 \div 2) \cdot 10^{-5}$ for high-class
devices to $(1 \div 10) \cdot 10^{-4}$ for simpler devices. Thus, it is important
for this paper that the inaccuracies can be considered to be known and perhaps
different for each data point.

In this paper the dependency of the refraction index of the wavelength is used
to illustrate the model generation algorithm proposed in \cite{Rudoy13}.
Its results are compared with the SVM regression. Moreover, the impact of the
complexity penalty on the quality and complexity of the generated models
is investigated. The problem of determining the model coefficients stability
in the general case of multivariate models is formally stated, a method for evaluating
solution stability is proposed, and the dependency of these characteristics of
the model hyperparameters is studied for the given case of determining the dispersion
of transparent polymers.

In the first part of this paper the dispersion problem is formally stated, and the
stability criteria is proposed. In the second part the algorithm proposed in\cite{Rudoy13},
used to generate the regression model, is briefly described. In the third part a
numeric method for stability estimation is proposed. In the fourth part the results
of the computational experiment are shown. In the experiment two polymers are considered,
for each of them 17 data points are given, corresponding to the refraction index at
different wavelengths.

\section{Problem statement}
\label{sect:prob_stat}

\paragraph{Regression problem.}
Let $D$ be the data set of $\ell$ refraction index measurements for a polymer:
$D = \{ \lambda_i, n_i \mid i \in \{ 1, \dots, \ell \} \}$, where $\lambda_i$ is the wavelength,
and $n_i$ is the measured refraction index.

It is required to find a function $\hat{f} = \hat{f}(\lambda)$, minimizing the standard
loss function, assuming Gaussian error:
\begin{equation}
  S(f, D) = \sum_{i = 1}^\ell (f(\lambda_i) - n_i)^2 \rightarrow \min_{f \in \mathcal{F}},
  \label{eq:s}
\end{equation}
where $\mathcal{F}$ is some set of superpositions from which an optimal one is to be found.

In other words,
\begin{equation}
  \hat{f}(\lambda) = \hat{f}_D(\lambda) = \mathop{\arg \min}\limits_{f \in \mathcal{F}} S(f, D).
  \label{eq:fhat}
\end{equation}

\paragraph{Stability estimation.}
We define the notion of the stability of some superposition $f$ in general case.
The stability describes the behavior of the coefficients of the superposition
$f$ during slight random variation of the source learning data set
$D = \{ \mathbf{x}_i, y_i \}$,
where $\mathbf{x}_i$ is the feature vector of $i$-th object measured during
the experiment, and $y_i$ is the corresponding measured value of the target function
to be recovered.

The loss function \eqref{eq:s} in this case is:
\begin{equation}
  S(f, D) = \sum_{i = 1}^\ell (f(\mathbf{x}_i) - y_i)^2 \rightarrow \min_{f \in \mathcal{F}}.
  \label{eq:s_common}
\end{equation}

We denote the matrix representing the data set as $X = \| x_{ij} \|$, where 
rows are feature vectors of the objects in $D$. In other words, $x_{ij}$
is the $j$-th component of the feature vector of the $i$-th object.

We consider the parameters vector
$\boldsymbol{\omega}_f = \{ \omega_i^f \mid i \in \{ 1, \dots, l_f \} \}$
of some superposition $f$: $f(\mathbf{x}) = f(\mathbf{x}, \boldsymbol{\omega}_f)$.
Let $\hat{\boldsymbol{\omega}}_f(D)$ be the parameters vector minimizing the
functional \eqref{eq:s_common} for some learning set $D = \{ \mathbf{x}_i, y_i \}$ and
function $f$ with fixed structure:
\[
  \hat{\boldsymbol{\omega}}_f(D) = \mathop{\arg \min}\limits_{\boldsymbol{\omega}_f} S(f, D).
\]

Let $\Sigma^{\mathbf{x}} = \| \sigma^{\mathbf{x}}_{ij} \|$ be the matrix of
standard deviations of independend variables, where $\sigma^{\mathbf{x}}_{ij}$
is the standard deviation of the $j$-th element of the feature vector
$\mathbf{x}_i$ of the $i$-th object of the learning set. Let $\boldsymbol{\sigma}^y$
be the vector of standard deviations of the dependent variable, where $\sigma^y_i$
is the standard deviation of the measured variable for the $i$-th object.
We then consider the modified learning set $\acute{D}$ derived from the
source data set $D$ by adding to its components some realizations of the
random variables from the Gaussian distribution with zero mean and 
deviations corresponding to $\Sigma^{\mathbf{x}}$ and $\boldsymbol{\sigma}^y$:
\begin{equation}
  \acute{D}(\Sigma^{\mathbf{x}}, \boldsymbol{\sigma}^y) = \{ \mathbf{x}_i + \boldsymbol{\xi}^{\mathbf{x}}_i, y_i + \xi^y_i \mid i \in 1, \dots, \ell; \boldsymbol{\xi}^{\mathbf{x}}_i \sim \mathcal{N}(0; \boldsymbol{\sigma}^{\mathbf{x}}_{i \cdot}); \xi^y_i \sim \mathcal{N}(0; \sigma^y_i) \}.
  \label{eq:d_acute}
\end{equation}

For this new learning set $\acute{D}$ we find the new parameters vector $\hat{\boldsymbol{\omega}}_f (\acute{D} (\Sigma^{\mathbf{x}}, \boldsymbol{\sigma}_y))$
for the superposition $f$ minimizing the functional \eqref{eq:s}:
\begin{equation}
  \hat{\boldsymbol{\omega}}_f (\acute{D} (\Sigma^{\mathbf{x}}, \boldsymbol{\sigma}_y)) = \mathop{\arg \min}\limits_{\boldsymbol{\omega}_{f_D} \in R^{\mid \hat{\boldsymbol{\omega}}_f \mid}} S (f_D (\cdot, \boldsymbol{\omega}_{f_D}), \acute{D} (\Sigma^{\mathbf{x}}, \boldsymbol{\sigma}_y)).
  \label{eq:hat_omega}
\end{equation}
Thus, $\hat{\boldsymbol{\omega}}_f (\acute{D} (\Sigma^{\mathbf{x}}, \boldsymbol{\sigma}_y))$ is
a random vector, and, consecutively
\[
  \Delta\hat{\boldsymbol{\omega}}_f(\acute{D} (\Sigma^{\mathbf{x}}, \boldsymbol{\sigma}_y) ) = \hat{\boldsymbol{\omega}}_f(D) - \hat{\boldsymbol{\omega}}_f (\acute{D} (\Sigma^{\mathbf{x}}, \boldsymbol{\sigma}_y))
\]
is also a random vector.

Let $\acute{\mathcal{D}}_N$ be a set of $N$ such modified learning sets, where each
set is obtained by adding a separate realization of the corresponding random variables
to the source data set:
\[
  \acute{\mathcal{D}}_N (\Sigma^{\mathbf{x}}, \boldsymbol{\sigma}_y) = \{ \acute{D}_1 (\Sigma^{\mathbf{x}}, \boldsymbol{\sigma}_y), \dots, \acute{D}_N (\Sigma^{\mathbf{x}}, \boldsymbol{\sigma}_y) \}.
\]
Let $\overline{\sigma}_i$ be the sample standard deviation of the $i$-th component of the
$\Delta\hat{\boldsymbol{\omega}}_f(\acute{D} (\Sigma^{\mathbf{x}}, \boldsymbol{\sigma}_y) )$
random vector on the $\acute{\mathcal{D}}_N (\Sigma^{\mathbf{x}}, \boldsymbol{\sigma}_y)$ set.
\begin{defin}
\emph{Relative stability} (or simply \emph{stability}) of the parameter
$\omega_i$ given $\acute{\mathcal{D}}_N (\Sigma^{\mathbf{x}}, \boldsymbol{\sigma}_y)$
and source learning set $D$ is the following vector of length $| \mathbf{x} | + 1$:
\begin{equation}
  \mathbf{T}^N_f(i) = \Big\{ \frac{\frac{\overline{\sigma}_i}{\hat{\omega}_i}}{r(\boldsymbol{\sigma}^\mathbf{x}_{\cdot 1}, \mathbf{x}_{\cdot 1})}, \dots, \frac{\frac{\overline{\sigma}_i}{\hat{\omega}_i}}{r(\boldsymbol{\sigma}^\mathbf{x}_{\cdot |\mathbf{x}|}, \mathbf{x}_{\cdot |\mathbf{x}|})}, \frac{\frac{\overline{\sigma}_i}{\hat{\omega}_i}}{r(\boldsymbol{\sigma}^y, \mathbf{y})} \Big\},
  \label{eq:t_rel}
\end{equation}
where $r(\boldsymbol{\alpha}, \mathbf{a}) = r(\frac{\alpha_1}{a_1}, \dots, \frac{\alpha_{|\boldsymbol{\alpha}|}}{a_{|\mathbf{a}|}})$ is
a function mapping a vector (comprised from quotients of the corresponding elements of vectors $\boldsymbol{\alpha}$ and $\mathbf{a}$),
to a scalar value.
\end{defin}

The function $r$ maps to a single scalar value a set of (perhaps different) ratios of standard
deviation of a measured variable to the value of that variable. The mapped scalar can be
viewed as some kind of a characteristic of those ratios. The function $r$ is chosen by the
experts based on the assumptions about the error distribution characteristics. For example,
in the case of polymers dispersion data the relative measurement error is constant as was
described in the introduction, thus the $r$ function may just choose any argument.

Each component of the $\mathbf{T}^N_f(i)$ vector describes the ratio between the standard deviation
of the $\hat{\omega}_i$ parameter (normalized by the value of that parameter) and the standard
deviation of the corresponding feature vector element (again, normalized by the value of that element).
For instance, if this ration is greater than one, then the uncertainity in determining the coefficient
grows faster than the measurement inaccuracy of the corresponding variable.

In the particular case of the dispersion regression considered in this paper, taking into account
the constant relative measurement error:
\[
  \mathbf{T}_f(i) = \Big\{ \frac{\frac{\overline{\sigma}_i}{\hat{\omega}_i}}{\frac{\sigma_n}{n}}, \frac{\frac{\overline{\sigma}_i}{\hat{\omega}_i}}{\frac{\sigma_{\lambda}}{\lambda}} \Big\}.
\]

Matrix comprised of vector columns $\mathbf{T}_f(i) \mid i \in \{ 1, \dots, l_f \}$
is called the \emph{stability} of the function $f$ and is denoted as $\mathbb{T}_f$.

In case of the dispersion regression, it is required to study the dependency of stability
$\mathbb{T}_{\hat{f}}$ as function of $\sigma_n$ and $\sigma_{\lambda}$.

\section{The algorithm for inductive models generation}

In this section we briefly describe the algorithm proposed in \cite{Rudoy13}.

Let $G = \{ g_1, \dots, g_k \}$ be the set of some elementary functions.
The set $\mathcal{F} = \{ f \}$ of generated models is first initialized
by random admissible superpositions of functions $g \in G$, taking their
arity, domain and codomain into account. Superpositions in $\mathcal{F}$
contain free variables corresponding to the components of the feature
vectors from the learning set, as well as constants which are subject to
optimization by the Levenberg-Marquardt procedure (according to functional
\eqref{eq:s}) on each algorithm step. Each superposition can also be modified
on each iteration in order to improve the quality $Q_f$ of best superpositions
in the set $\mathcal{F}$.

The quality $Q_f$ of the model $f$ is defined by the error on the learning
set as well as the structural complexity of the superposition according to the
following:
\begin{equation}
  Q_f = \frac{1}{1 + S(f)} \left(\alpha + \frac{1 - \alpha}{1 + \text{exp} (C_f - \tau)}\right),
  \label{eq:s_f}
\end{equation}
where:
\begin{itemize}
  \item[] $S(f)$ is the value of the loss functional \eqref{eq:s} on the learning set $D$;
  \item[] $C_f$ is the complexity of the superposition $f$ defined by the number of
	elementary functions, free variables and constants;
  \item[] $\alpha, 0 \ll \alpha < 1$ adjusts the penalty of excessive model complexity
	(bigger $\alpha$ values prefer more complex but more precise models, while smaller
	choose simpler ones);
  \item[] $\tau$ defines the desired complexity of the model, after which it is considered
	excessive.
\end{itemize}

The second multiplier in \eqref{eq:s_f} is the penalty for excessive model complexity,
which mitigates overfitting and allows obtaining simpler superpositions at the cost of
bigger error on the learning set. The primary hypothesis is that simpler superpositions
with slightly bigger learning set errors generalize better.

It is worth noting that $\alpha$ and $\tau$ are chosen by experts.

So, the initial problem of minimizing the functional \eqref{eq:s} is replaced by
the problem of minimizing \eqref{eq:s_f}:
\begin{equation}
  Q_f = \frac{1}{1 + S(f)} \left(\alpha + \frac{1 - \alpha}{1 + \text{exp} (C_f - \tau)}\right) \rightarrow \min_{f \in \mathcal{F}}.
  \label{eq:s_f_min}
\end{equation}

\section{Stability estimation method}

In order to estimate the stability $\mathbb{T}_{\hat{f}}$ of some model $\hat{f}$
which a solution of \eqref{eq:s_f}, the structure of the model $\hat{f}$ is fixed
and the standard deviation of its parameters is studied as the function of the
standard deviation of a noise in the learning set, as proposed in section
\ref{sect:prob_stat}.

In other words, some values for $\sigma_{\lambda}$ and $\sigma_n$ are chosen,
then the modified learning set $\acute{D}(\sigma_n, \sigma_{\lambda})$ is
generated for the chosen values according to \eqref{eq:d_acute}. The new
parameters vector is then calculated which minimizes \eqref{eq:s} on the
modified learning set $\acute{D}(\sigma_n, \sigma_{\lambda})$ according to
\eqref{eq:hat_omega}.

This procedure is repeated multiple times for each given pair of $\sigma_{\lambda}$ and $\sigma_n$
until some stop condition is reached (like the number of iterations), after which
empirical value for $\mathbb{T}_{\hat{f}}$ is computed.

By performing the above steps for different $\sigma_{\lambda}$ and $\sigma_n$,
it is possible to estimate the dependency of the superposition coefficients
standard deviation of the parameters $\sigma_{\lambda}$ and $\sigma_n$ of the noise.

It is physically sensible to expect this dependency to be smooth, while extremely
non-smooth dependency means an erroneously chosen superposition and can also be
a sign of overfitting: the less the coefficients depend on the random error
in the data, the better generalization is.

Moreover, different superpositions can be compared according to the proposed
stability criteria in addition to the complexity and error criteria. In some
applications the stability can even be more important than the error on the
data set.

\section{Computational experiment}

The data used in this section are the measurements of the refraction index
of transparent polymers as a function of wavelength. Two different polymers
are considered, each of them having 17 data points corresponding to the
refraction index at different wavelengths. The values of the measurements
are shown in table \ref{tabl:source_data}.

\begin{table}[h]
  \footnotesize
  \caption{Measured refraction indexes at different wavelengths.}
  \centering
  \begin{tabular}{r | r | r}
	$\lambda$, nm	& Polymer 1 & Polymer 2 \\ \hline
	435.8 & 1.36852 & 1.35715 \\
	447.1 & 1.36745 & 1.35625 \\
	471.3 & 1.36543 & 1.35449 \\
	486.1 & 1.36446 & 1.35349 \\
	501.6 & 1.36347 & 1.35275 \\
	546.1 & 1.36126 & 1.35083 \\
	577.0 & 1.3599 & 1.34968 \\
	587.6 & 1.3597 & 1.34946 \\
	589.3 & 1.35952 & 1.34938 \\
	656.3 & 1.35767 & 1.34768 \\
	667.8 & 1.35743 & 1.34740 \\
	706.5 & 1.35652 & 1.34664 \\
	750 & 1.35587 & 1.34607 \\
	800 & 1.35504 & 1.34544 \\
	850 & 1.3544 & 1.34487 \\
	900 & 1.35403 & 1.34437 \\
	950 & 1.35364 & 1.34407 \\
  \end{tabular}
  \label{tabl:source_data}
\end{table}

The dispersion of both polymers is assumed to be described by the functional dependency
of the same structure, as it obeys the same physical laws. Because of this, firstly the
model $\hat{f}$ is chosen which minimizes \eqref{eq:s_f} for the first polymer, and then
for each of the polymers optimal parameter vectors $\hat{\boldsymbol{\omega}}_{\hat{f}}$
are found for the given model, and their stability is estimated.

The data set was not splitted to learning set and control set, as overfitting was
mitigated by the complexity penalty.

Physical considerations show \cite{Serova11} that dispersion should be a sum of even
powers of the wavelength, so the elementary function set consists of the functions
\[
  g_3(\lambda, p) = \frac{1}{\lambda^{2p}},
\]
in addition to standard addition and multiplication operations:
\[
  g_1(x_1, x_2) = x_1 + x_2,
\]
\[
  g_2(x_1, x_2) = x_1 x_2.
\]

During the experiment constants with absolute value less than $10^{-7}$
were zeroed.

The algorithm described above generated the following superposition at $\alpha = 0.05$, $\tau = 10$:
\begin{equation}
  f(\lambda) = 1.3495 + \frac{3.5465 \cdot 10^3}{\lambda^2} + \frac{2.023 \cdot 10^3}{\lambda^4}.
  \label{eq:res_0}
\end{equation}
The complexity of this model is $13$, MSE is $2.4 \cdot 10^{-8}$ and $Q_f \approx 0.095$.
Wavelengths are assumed to be in nanometers.

It is worth noting that only two first terms are considered in practical applications, while
higher powers are neglected. The value of the last term in \eqref{eq:res_0}
agrees with this practice.

\paragraph{Complexity penalty.}

The effect of adding odd powers to the elementary function set is studied here by replacing
$g_3$ with 
\[
  g_3(\lambda, p) = \frac{1}{\lambda^p}.
\]

With the same parameters $\alpha = 0.05$ and $\tau = 10$ the resulting function is still
\eqref{eq:res_0}.

Increasing $\tau$ up to 30 results in the following model:
\begin{equation}
  n(\lambda) = 1.34 + \frac{11.6}{\lambda} + \frac{17.37}{\lambda^2} + \frac{0.0866}{\lambda^3} + \frac{2.95 \cdot 10^{-4}}{\lambda^4} + \frac{8.54 \cdot 10^{-7}}{\lambda^5}.
  \label{eq:res_incorrect}
\end{equation}
Its complexity is $31$, MSE is $\approx 3.9 \cdot 10^{-9}$ and $Q_f \approx 0.31$.

In other words, bigger target complexity (expressed by $\tau$) leads to selecting
more complex (and, in this case, physically incorrect) model with smaller mean square error.

Naturally, excessive values of $\tau$ lead to overfitting.

\paragraph{SVM.}

SVM with RBF kernel \cite{Vapnik79} was used as baseline algorithm. The $\gamma$ kernel
parameter was selected by crossvalidation. The best result was a combination of 15
support vectors with $\gamma \approx 2 \cdot 10^{-6}$. MSE during 2-fold crossvalidation
was $8.96 \cdot 10^{-8}$. The resulting model, though, is uninterpretable.

\paragraph{Model stability.}

In order to estimate the stability, the structure of \eqref{eq:res_0} had been fixed as
\[
  f(\lambda) = \omega_1 + \frac{\omega_2}{\lambda^2} + \frac{\omega_3}{\lambda^4},
\]
and the dependency of standard deviation of $\omega_1$, $\omega_2$ and $\omega_3$
of standard deviation of a gaussian noise was studied by the method proposed above.
The stop criteria was reaching $10^4$ iterations for each pair of $(\sigma_{\lambda}, \sigma_n)$.

The standard deviation surfaces of each coefficient of each polymer are shown in
table \ref{tabl:res_even}.

\begin{table}[h]
  \centering
  \footnotesize
  \caption{Standard deviation for \eqref{eq:res_0}.}
  \begin{tabular}{l | c c c}
	  & $\omega_1$ & $\omega_2$ & $\omega_3$ \\ \hline
	\begin{rotate}{90}Polymer 1\end{rotate} &	\includegraphics[scale=0.4]{figs/even/p1.txt_coeff0.dat.eps} & \includegraphics[scale=0.4]{figs/even/p1.txt_coeff1.dat.eps} & \includegraphics[scale=0.4]{figs/even/p1.txt_coeff2.dat.eps} \\
	\begin{rotate}{90}Polymer 2\end{rotate} &	\includegraphics[scale=0.4]{figs/even/p2.txt_coeff0.dat.eps} & \includegraphics[scale=0.4]{figs/even/p2.txt_coeff1.dat.eps} & \includegraphics[scale=0.4]{figs/even/p2.txt_coeff2.dat.eps}
  \end{tabular}
  \label{tabl:res_even}
\end{table}

\begin{table}[h]
  \centering
  \footnotesize
  \caption{Coefficients of model \eqref{eq:res_0} and their relative residual.}
  \begin{tabular}{l | c | c | c | c}
				& $\omega_1$				& $\omega_2$				& $\omega_3$				& MSE	\\ \hline
    Polymer 1	& 1.34946				& 3558.95				& 1924.33				& $2.2 \cdot 10^{-8}$		\\
    Polymer 2	& 1.34047				& 3118.84				& 1578.59				& $1.4 \cdot 10^{-8}$		\\
	Residual		& $6.71 \cdot 10^{-3}$	& $1.41 \cdot 10^{-1}$	& $2.2 \cdot 10^{-1}$	&	\\
  \end{tabular}
  \label{tabl:res_even_coeffs}
\end{table}

\begin{table}[h]
  \centering
  \footnotesize
  \caption{Standard eviation values for coefficients of \eqref{eq:res_0} for the first polymer for some selected noise parameters.}
  \begin{tabular}{l | c | c | c}
	$\omega_i$	& $\frac{\sigma_{\lambda}}{\lambda} = 2 \cdot 10^{-4}; \frac{\sigma_n}{n} = 2 \cdot 10^{-5}$	& $ \frac{\sigma_{\lambda}}{\lambda} = 6 \cdot 10^{-4}; \frac{\sigma_n}{n} = 6 \cdot 10^{-5} $	& $ \frac{\sigma_{\lambda}}{\lambda} = 9 \cdot 10^{-4}; \frac{\sigma_n}{n} = 2 \cdot 10^{-4} $ \\ \hline
	1		& $1.22 \cdot 10^{-5}$																			& $ 3.59 \cdot 10^{-5} $																		& $ 1.19 \cdot 10^{-4} $		\\
	2		& $1.48 \cdot 10^{-3}$																			& $ 4.38 \cdot 10^{-3} $																		& $ 1.44 \cdot 10^{-2} $		\\
  \end{tabular}
  \label{tabl:res_even_stddev}
\end{table}

The graphs show that the inaccuracy in measuring the wavelength does not significantly affect
first and second coefficients in the region of interest. At the same time, their standard deviation
depends on the standard deviation of the refraction index almost linearly.

These results can be interpreted in the following way: during experiment planning most attention
should be paid to maximizing the certainity in measuring the refraction index, while the wavelength
can be measured quite inaccurately with errors up to few nanometers. Moreover, the suggested method
directly shows how the uncertainity in determining the coefficients depends on the measurement errors
of different parameters.

It is fundamentally important that the standard deviation of the parameters of the model \eqref{eq:res_0}
are considerably smaller than the difference between the parameters for two polymers (as shown
by tables \ref{tabl:res_even_coeffs} and \ref{tabl:res_even_stddev}), which means that the polymers
can be separated by such measurements even by an imprecise refractometer.

\paragraph{Stability of the overfitted model.}

The stability of the model \eqref{eq:res_incorrect} is studied analogously. Standard deviation
graphs for the first three coefficients are shown in table \ref{tabl:res_incorrect}.

\begin{table}[h]
  \centering
  \footnotesize
  \caption{Standard deviation for \eqref{eq:res_incorrect}.}
  \begin{tabular}{l | c c c}
	  & $\omega_1$ & $\omega_2$ & $\omega_3$ \\ \hline
	\begin{rotate}{90}Polymer 1\end{rotate} &	\includegraphics[scale=0.4]{figs/all/p1.txt_coeff0.dat.eps} & \includegraphics[scale=0.4]{figs/all/p1.txt_coeff1.dat.eps} & \includegraphics[scale=0.4]{figs/all/p1.txt_coeff2.dat.eps} \\
	\begin{rotate}{90}Polymer 2\end{rotate} &	\includegraphics[scale=0.4]{figs/all/p2.txt_coeff0.dat.eps} & \includegraphics[scale=0.4]{figs/all/p2.txt_coeff1.dat.eps} & \includegraphics[scale=0.4]{figs/all/p2.txt_coeff2.dat.eps}
  \end{tabular}
  \label{tabl:res_incorrect}
\end{table}

The graphs show that standard deviation values for \eqref{eq:res_incorrect} are considerably higher
than the corresponding ones for \eqref{eq:res_0}. In particular, the second, third and fourth
coefficients have standard deviation orders of magnitude higher than their corresponding values.

These results may be a sign of overfitting, and that the resulting model can't be used to
describe the physical process measured, not to mention separating two polymers in their mixture.

\section{Convergence to the linear case.}

The case of linear regression is considered:
\[
  y = ax + b.
\]
Taking the measurement errors into account:
\[
  y_i = ax_i + b + \xi_i \mid i \in \{ 1, \dots, n \},
\]
where the errors $\xi_i$ are independent, and $E(\xi_i) = 0; D(\xi_i) = \sigma^2$ \cite{Vatunin05}.
In other words, the error doesn't depend on the measurement, and the dependent variable
is measured precisely.

Transitioning to the following presentation:
\[
  y_i = a(x_i - \overline{x}) + b + \xi_i \mid i \in \{ 1, \dots, n \},
\]
results in $a$ and $b$ being independent normally distributed random variables, and
their dispersions can be calculated according to \cite{Vatunin05}:
\begin{equation}
  \label{eq:classic_da}
  D(a) = \frac{\sigma^2}{\sum_{i = 1}^n (x_i - \overline{x})^2}.
\end{equation}
\begin{equation}
  \label{eq:classic_db}
  D(b) = \frac{\sigma^2}{n}.
\end{equation}

Next the results obtained by the proposed method are compared to the ones resulting
from\eqref{eq:classic_da} and \eqref{eq:classic_db}. For this, the relative difference
between these values and empiric standard deviations is considered as a function of
number of iterations $N$:
\[
  \delta_1 = \frac{| \mathbf{T}^N_y(1) - D(a) |}{D(a)},
\]
\[
  \delta_2 = \frac{| \mathbf{T}^N_y(2) - D(b) |}{D(b)}.
\]

Corresponding graphs for the function $y = 2x + 1 + \xi_i$ with $x \in [0; 10]$,
$n = 10$ sample points and $D(\xi_i) = 10$ are shown on fig. \ref{fig:classic_var10_n10}.
Particularly, the fig. \ref{fig:classic_var10_n10_begin} shows the initial part of the
graph for $N$ smaller than $5 \cdot 10^5$, the fig. \ref{fig:classic_var10_n10_middle}
shows the part for $N$ between $5 \cdot 10^5$ and $10^7$, and fig.
\ref{fig:classic_var10_n10_end} represents the convergence on big $N$ (from $10^7$ to $10^8$).

Analogous graphs are also shown for $n = 10$ and $D(\xi_i) = 1$, and $n = 50$ and $D(\xi_i) = 1$,
on fig. \ref{fig:classic_var1_n10} and \ref{fig:classic_var1_n50} respectively.

The graphs show that the relative difference stabilizes around $(1.5 \div 3) \cdot 10^6$
iterations and doesn't explicit dependence on the number of sample points or standard
deviation of the error.

\begin{figure}[h!]
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figs/classic/linear_log_1x_2_samples_10_variance_10_norm.log_0_500.eps}
    \caption{$N \in [0;~5 \cdot 10^5]$}
    \label{fig:classic_var10_n10_begin}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figs/classic/linear_log_1x_2_samples_10_variance_10_norm.log_500_10000.eps}
    \caption{$N \in [5 \cdot 10^5;~10^7]$}
    \label{fig:classic_var10_n10_middle}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figs/classic/linear_log_1x_2_samples_10_variance_10_norm.log_end.eps}
    \caption{$N \in [10^7;~10^8]$}
    \label{fig:classic_var10_n10_end}
  \end{subfigure}
  \caption{Dependence of $\delta$ on $N$ with $D(\xi) = 10$ and $n = 10$.}
  \label{fig:classic_var10_n10}
\end{figure}

\begin{figure}[h!]
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figs/classic/linear_log_1x_2_samples_10_variance_1_norm.log_0_500.eps}
    \caption{$N \in [0;~5 \cdot 10^5]$}
    \label{fig:classic_var1_n10_begin}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figs/classic/linear_log_1x_2_samples_10_variance_1_norm.log_500_10000.eps}
    \caption{$N \in [5 \cdot 10^5;~10^7]$}
    \label{fig:classic_var1_n10_middle}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figs/classic/linear_log_1x_2_samples_10_variance_1_norm.log_end.eps}
    \caption{$N \in [10^7;~10^8]$}
    \label{fig:classic_var1_n10_end}
  \end{subfigure}
  \caption{Dependence of $\delta$ on $N$ with $D(\xi) = 1$ and $n = 10$.}
  \label{fig:classic_var1_n10}
\end{figure}

\begin{figure}[h!]
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figs/classic/linear_log_1x_2_samples_50_variance_1_norm.log_0_500.eps}
    \caption{$N \in [0;~5 \cdot 10^5]$}
    \label{fig:classic_var1_n50_begin}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figs/classic/linear_log_1x_2_samples_50_variance_1_norm.log_500_10000.eps}
    \caption{$N \in [5 \cdot 10^5;~10^7]$}
    \label{fig:classic_var1_n50_middle}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figs/classic/linear_log_1x_2_samples_50_variance_1_norm.log_end.eps}
    \caption{$N \in [10^7;~10^8]$}
    \label{fig:classic_var1_n50_end}
  \end{subfigure}
  \caption{Dependence of $\delta$ on $N$ with $D(\xi) = 1$ and $n = 50$.}
  \label{fig:classic_var1_n50}
\end{figure}

\section{Заключение}

Предложенный в \cite{Rudoy13} алгоритм позволяет получить интерпретируемую аналитическую
формулу, описывающую зависимость коэффициента преломления среды от длины волны.
Введенный штраф за сложность позволяет избежать переобучения без использования методов
вроде скользящего контроля, и, таким образом, отпадает необходимость в контрольной выборке.

Хотя другие алгоритмы, такие как SVM-регрессия, могут демонстрировать более высокое
качество приближения данных, их результаты неинтерпретируемы и не защищены от переобучения
<<по построению>>, поэтому требуют разделения выборки на обучающую и контрольную. Кроме
того, их структурные параметры так же требуют оценки по методам вроде кросс-валидации.

Предложенный в настоящей работе метод оценки стабильности решения позволяет исследовать вклад различных
членов результирующей суперпозиции и зависимость изменения этих членов от
случайных шумов во входных данных. В частности, в прикладных областях данный метод позволяет
выявить, какие именно элементы признакового описания объектов в генеральной совокупности
наиболее чувствительны к шуму. Кроме того, для корректных с экспертной точки зрения
решений оказывается, что они стабильны, в то время как некорректные результаты нестабильны.

\FloatBarrier

\bibliographystyle{babunsrt-lf}
%\bibliographystyle{babunsrt}
%\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document}
