\documentclass[11pt,a4paper]{article}
\usepackage{a4wide}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage{graphics,graphicx,epsfig}
\usepackage{amssymb,amsfonts,amsthm,amsmath,mathtext,cite,enumerate,float}
\usepackage[english,russian]{babel}
\usepackage[all]{xy}
\usepackage{morefloats}
\usepackage{pgf}
\usepackage[debug,outputdir={docgraphs/}]{dot2texi}
\usepackage{tikz}
\usepackage{scalefnt}
\usepackage{float}
\usepackage{verbatim}
\usepackage{placeins}
\usepackage{url}
\usepackage{babelbib}

\makeatletter
\def\@settitle{\begin{center}%
    \baselineskip14\p@\relax
    \bfseries
    \@title
  \end{center}%
}

\makeatother

\newcommand{\bomega}{\boldsymbol{\omega}}

\begin{document}

\begin{center}
  МОДИФИКАЦИЯ АЛГОРИТМА ЛЕВЕНБЕРГА-МАРКВАРДТА ДЛЯ ЗАДАЧ НЕЛИНЕЙНОЙ РЕГРЕССИИ С УЧЕТОМ
  ПОГРЕШНОСТЕЙ КАК ЗАВИСИМЫХ, ТАК И НЕЗАВИСИМЫХ ДАННЫХ

  \bigskip
  Г.\,И.~Рудой
\end{center}

\section{Введение}

Для известной задачи нахождения оптимальных коэффициентов некоторой фиксированной
регрессионной модели, представленной в виде формулы, по набору экспериментальных
данных широко применяется алгоритм Левенберга-Марквардта
\cite{Marquardt1963Algorithm}, минимизирующий сумму квадратов отклонений
экспериментальных точек от регрессионной кривой. Однако, данный алгоритм построен
и статистически обоснован в предположении о нормальности распределения регрессионных
остатков и точно измеренных независимых переменных~--- иными словами, учитываются
и рассматриваются только ошибки измерения зависимой переменной. Более того,
предполагается, что ошибки для всех точек принадлежат одному и тому же распределению
с одними и теми же параметрами.

В большинстве естественнонаучных приложений это предположение не выполняется. Например, в задаче
нахождения дисперсионной зависимости прозрачного полимера (то есть, зависимости
коэффициента преломления $n$ от длины волны $\lambda$) погрешности измерения
различных физических параметров, вообще говоря, различны. Так, например, если
для измерения длины волны $\lambda$ используется дифракционная решетка, то постоянной
является относительная погрешность определения длины волны
$\frac{\sigma_{\lambda_i}}{\lambda_i} \approx \text{const}$, и, следовательно,
погрешность определения длины волны зависит от самой длины волны.

Таким образом, возникает задача поиска оптимальных коэффициентов регрессионной
формулы с учетом отличающихся погрешностей различных экспериментальных точек.
Для некоторых частных случаев эта задача уже была решена: например, 
в работе \cite{...} вводится предположение, что зависимые переменные $y_i$ измеряются
неточно, и каждая переменная $y_i$ имеет свою собственную погрешность измерения
$\sigma_{y_i}$. Далее в \cite{...} показывается, что обычный функционал суммы квадратов
регрессионных остатков, где каждый остаток нормирован на соответствующую величину
$\sigma_{y_i}^2$, корректен и статистически состоятелен.

В настоящей работе рассмотрена более общая ситуация, в которой независимые
переменные также определяются неточно в процессе эксперимента, и каждая переменная
имеет свою собственную
погрешность измерения. Предлагается функционал качества и модифицированный алгоритм
Левенберга-Марквардта, позволяющий найти оптимальные коэффициенты регрессионной
формулы и опирающийся на классический алгоритм
Левенберга-Марквардта (в дальнейшем будем называть их мАЛМ и АЛМ соответственно).
Доказывается сходимость модифицированного алгоритма и приводятся
результаты на экспериментальных данных по измерению параметров усиливающей
среды лазерного излучателя.

\section{Постановка задачи}

Дана обучающая выборка $D = \{ \mathbf{x}_i, y_i \} | i \in \{ 1, \dots, \ell \}, x_i \in \mathbb{R}^m, y_i \in \mathbb{R}$.
Для каждой зависимой переменной переменной $y_i$ известно
стандартное отклонение ошибки ее измерения $\sigma_{y_i}$, а для соответствующего
вектора независимых переменных $\mathbf{x}_i$ аналогично известны стандартные
отклонения его компонент $\sigma_{x_{ij}} | j \in \{ 1, \dots, m \}$.
Пусть, кроме того, дана некоторая параметрическая регрессионная модель
$y = f (\mathbf{x}, \bomega)$.

Для удобства введем вектор, составленный из ошибок измерений зависимых переменных
$\sigma_{y_i}$:
\[
  \boldsymbol{\sigma}_y = \{ \sigma_{y_1}, \dots, \sigma_{y_{\ell}} \}.
\]

Аналогично введем матрицу, составленную из ошибок измерений компонент
независимых переменных $\sigma_{x_{ij}}$:
\[
  \Sigma_x = \| \sigma_{x_{ij}} \| | i \in \{ 1, \dots, \ell \}, j \in \{ 1, \dots, m \}.
\]

Требуется построить функционал ошибки $S(\bomega)$ вектора параметров
$\bomega$ модели $f$, учитывающий ошибки измерений $\sigma_{y_i}$ и
$\sigma_{x_{ij}}$:

\begin{equation}
  S(\bomega) = S(\bomega, \boldsymbol{\sigma}_y, \Sigma_x),
\end{equation}

и, кроме того, найти вектор параметров $\omega$, минимизирующий функционал
$S$:
\begin{equation}
  \hat{\bomega} = \mathop{\arg \min}\limits_{\bomega} S(\bomega)
\end{equation}

\section{Модифицированный функционал качества}

Воспользуемся следующим естественным качественным физическим соображением:
чем больше погрешность определения зависимой и независимых переменных
для некоторой экспериментальной точки, тем меньше соответствующий
регрессионный остаток должен учитываться при оптимизации параметров модели.

Рассмотрим для простоты изложения случай одной независимой переменной:
$x \in \mathbb{R}$. Введем следующее определение расстояния $\rho(x, i)$
от точки $(x_i, y_i)$ из обучающей выборки до некоторой точки
$(x, f(x, \bomega))$ на кривой, описываемой регрессионной моделью:
\begin{equation}
  \rho(x, i) = \frac{(x_i - x)^2}{\sigma_{x_i}^2} + \frac{(y_i - y)^2}{\sigma_{y_i}^2},
  \label{eq:dist0}
\end{equation}
где $y = f(x, \bomega)$.

Непосредственное точное определение расстояния представляется отдельной
сложной вычислительной задачей, однако, в подавляющем большинстве
практических приложений регрессионные зависимости достаточно
гладкие, а погрешности измерения достаточно малы. Пользуясь этим,
линеаризуем $f(x, \bomega)$ в окрестности точки $x_i$:
\begin{equation}
  f(x, \bomega) = f(x_i, \bomega) + (x - x_i) \frac{\partial f}{\partial x}(x_i, \bomega).
  \label{eq:f_linear}
\end{equation}
Введем для удобства обозначение $k = \frac{\partial f}{\partial x}(x_i, \bomega)$.

Тогда расстояние \eqref{eq:dist0} можно выразить через линеаризованную функцию
\eqref{eq:f_linear} следующим образом:
\begin{equation}
  \rho(x, i) = \frac{(x_i - x)^2}{\sigma_{x_i}^2} + \frac{(y_i - f(x_i, \bomega) - k (x - x_i))^2}{\sigma_{y_i}^2}.
  \label{eq:dist_linear}
\end{equation}

Минимизируя это выражение, находим выражение для расстояния от
точки $(x_i, y_i)$ из обучающей выборки до линеаризованной в ее
окрестности экспериментальной зависимости:
\begin{equation}
  \rho(x, i) = \frac{(y_i - f(x_i, \bomega))^2}{\sigma^2_{y_i} + k^2 \sigma^2_{x_i}}.
  \label{eq:rho_univar}
\end{equation}

Аналогичным образом можно получить выражение для случая, когда $\mathbf{x}$~---
вектор $m$-мерном пространстве $\mathbb{R}^m$:
\[
  \rho(\mathbf{x}, i) = \frac{(y_i - f(\mathbf{x}_i, \bomega))^2}{\sigma_{y_i}^2 + \sum_{j = 1}^m (\frac{\partial f}{\partial x_j}(\mathbf{x}_i, \bomega))^2 \sigma^2_{x_{ij}}}.
\]

Таким образом, функционал, минимизирующий сумму введеных согласно \eqref{eq:dist0}
расстояний, выглядит следующим образом:
\begin{equation}
  S(\bomega) = \sum_{i = 1}^\ell \frac{(y_i - f(\mathbf{x}_i, \bomega))^2}{\sigma_{y_i}^2 + \sum_{j = 1}^m (\frac{\partial f}{\partial x_j}(\mathbf{x}_i, \bomega))^2 \sigma^2_{x_{ij}}}.
  \label{eq:s}
\end{equation}

Отметим следующие наблюдения:
\begin{itemize}
  \item Функционал \eqref{eq:s} соответствует классической сумме квадратов регрессионных
	остатков при условии нормировки квадрата каждого остатка на на сумму квадратов погрешности
	определения зависимой величины $\sigma_{y_i}$ и произведения частной производной
	регрессионной модели по $j$-ой компоненте вектора независимых величин на погрешность
	определения соответствующей компоненты $\sigma_{x_{ij}}$.

  \item При прочих равных условиях в выражении для расстояния \eqref{eq:rho_univar} и,
	соответственно, в функционале \eqref{eq:s} сильнее учитываются те точки, в которых
	производная регрессионной модели $\frac{\partial f}{\partial x_j}$ по соответствующей
	компоненте $x_j$ меньше, что соответствует физической интуиции: чем меньше наклон
	регрессионной зависимости в окрестности данной точки, тем меньше влияние неточного
	измерения соответствующей независимой переменной на значение регрессионной зависимости
	в этой точке.

  \item Кроме того, если все независимые переменные измерены точно, то есть,
	$\forall i, j : \sigma_{x_ij} = 0$, то предложенный функционал переходит в предложенный
	в \cite{...}. Если же, кроме того, все зависимые переменные имеют одну и ту же погрешность,
	то предложенный функционал переходит в известную сумму квадратов регрессионных остатков
	(с точностью до некоторого множителя $\sigma_y$).
\end{itemize}

\section{Модифицированный алгоритм Левенберга-Марквардта}

Для минимизации функционала \eqref{eq:s} предлагается следующий итеративный алгоритм,
предназначенный для использования с уже имеющимися реализациями АЛМ
\footnote{Предполагается, что реализация АЛМ <<принимает на вход>> массив значений $y_i$,
функцию вычисления значения $f$ в точках $\mathbf{x}_i$ с вектором параметров $\bomega$,
и критерий останова в виде числа итераций. Примером такой реализации, использовавшейся
авторами, может служить \cite{dlib09}.}:
\begin{enumerate}
  \item Выбирается некоторое начальное приближение вектора параметров $\bomega$.
  \item Для каждой пары $(\mathbf{x}_i, y_i)$ из обучающей выборки рассчитывается значение
	частной производной $\frac{\partial f}{\partial x}$ в точке $(\mathbf{x}_i, \bomega)$.
  \item Каждое значение зависимой переменной $y_i$ и значение функции $f(\mathbf{x}_i, \bomega)$
	нормируется на соответствующую величину
	\[
	  \sigma_{y_i}^2 + \sum_{j = 1}^m (\frac{\partial f}{\partial x_j}(\mathbf{x}_i, \bomega))^2 \sigma^2_{x_{ij}}.
	\]
  \item Выполняется итерация АЛМ для таким образом модифицированных значений функции $f$
    и зависимых переменных $y_i$, таким образом получается новое значение вектора $\bomega$.
  \item Если критерий останова мАЛМ не достигнут, алгоритм продолжает выполнение с пункта 2.
\end{enumerate}

Отметим следующее:
\begin{itemize}
  \item Критерием останова мАЛМ могут служить обычные критерии вроде достижения некоторого
    числа итераций, нормы изменения вектора $\bomega$, и т.~п.
  \item Если известно, что производная $\frac{\partial f}{\partial x}$ достаточно гладка
	в окрестности $(\mathbf{x}_i, \bomega) \mid i \in \{ 1, \dots, \ell \}$, на шаге
	4 алгоритма мАЛМ представляется разумным выполнить сразу несколько итераций
	классического АЛМ во избежание потенциально ресурсоемкого пересчета производных и
	перенормировки значений $y_i$ и $f$.
\end{itemize}

Сходимость предложенного алгоритма можно показать, сведя его к классическому АЛМ. Для этого
вместо объектов $(\mathbf{x}_i, y_i)$ в обучающей выборке будем формально рассматривать
объекты $(\tilde{\mathbf{x}}_i, \tilde{y}_i)$, где $\tilde{y}_i = 0$, а
$\tilde{\mathbf{x}}_i$~--- вектор $\mathbf{x}_i$ с дополнительно приписанным к нему
значением $y_i$. Примем
\[
  \tilde{f}(\tilde{\mathbf{x}}_i, \bomega) = \frac{f(\mathbf{x}_i, \bomega) - y_i}{\sigma_{y_i}^2 + \sum_{j = 1}^m (\frac{\partial f}{\partial x_j}(\mathbf{x}_i, \bomega))^2 \sigma^2_{x_{ij}}}.
\]
Тогда минимизация функционала \eqref{eq:s} возможна средствами классического АЛМ, так
как легко видеть, что \eqref{eq:s} в этом случае эквивалентен
\[
  S(\bomega) = \sum_{i = 1}^\ell (\tilde{y}_i - \tilde{f}(\tilde{\mathbf{x}}_i, \bomega))^2.
\]

Кроме того, при возможности выполнить аналитическое дифференцирование функции $f$, этот
метод показывает еще один способ практической минимизации функционала \eqref{eq:s} без
постоянной корректировки значений $y_i$ и $f$, как в предложенном выше алгоритме.

\FloatBarrier

\bibliographystyle{babunsrt-lf}
%\bibliographystyle{babunsrt}
%\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document}
