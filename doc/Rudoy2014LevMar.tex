\documentclass[11pt,a4paper]{article}
\usepackage{a4wide}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage{graphics,graphicx,epsfig}
\usepackage{amssymb,amsfonts,amsthm,amsmath,mathtext,cite,enumerate,float}
\usepackage[english,russian]{babel}
\usepackage[all]{xy}
\usepackage{morefloats}
\usepackage{pgf}
\usepackage[debug,outputdir={docgraphs/}]{dot2texi}
\usepackage{tikz}
\usepackage{scalefnt}
\usepackage{float}
\usepackage{verbatim}
\usepackage{placeins}
\usepackage{url}
\usepackage{babelbib}

\makeatletter
\def\@settitle{\begin{center}%
    \baselineskip14\p@\relax
    \bfseries
    \@title
  \end{center}%
}

\makeatother

\newcommand{\bomega}{\boldsymbol{\omega}}

\begin{document}

\begin{center}
  МОДИФИКАЦИЯ АЛГОРИТМА ЛЕВЕНБЕРГА-МАРКВАРДТА ДЛЯ ЗАДАЧ НЕЛИНЕЙНОЙ РЕГРЕССИИ С УЧЕТОМ
  ПОГРЕШНОСТЕЙ КАК В ЗАВИСИМЫХ, ТАК И В НЕЗАВИСИМЫХ ДАННЫХ

  \bigskip
  Г.\,И.~Рудой
\end{center}

\section{Введение}

Для известной задачи нахождения оптимальных коэффициентов некоторой фиксированной
регрессионной модели, представленной в виде формулы, по набору экспериментальных
данных широко применяется алгоритм
Левенберга-Марквардта \cite{Marquardt1963Algorithm}. Однако, данный алгоритм построен
и статистически обоснован в предположении о нормальности распределения регрессионных
остатков и точно измеренных независимых переменных~--- иными словами, учитываются
и рассматриваются только ошибки измерения зависимой переменной. Более того,
предполагается, что ошибки для всех точек принадлежат одному и тому же распределению
с одними и теми же параметрами.

В ряде физических приложений это предположение не выполняется. Например, в задаче
нахождения дисперсионной зависимости прозрачного полимера (то есть, зависимости
коэффициента преломления $n$ от длины волны $\lambda$) погрешности измерения
различных физических параметров, вообще говоря, различны. Так, например, если
для измерения длины волны $\lambda$ используется дифракционная решетка, то постоянной
является относительная погрешность определения длины волны
$\frac{\sigma_{\lambda_i}}{\lambda_i} \approx \text{const}$, и, следовательно,
погрешность определения длины волны зависит от самой длины волны.

Таким образом, возникает задача поиска оптимальных коэффициентов регрессионной
формулы с учетом отличающихся погрешностей различных экспериментальных точек.
Для некоторых частных случаев эта задача уже была решена: например, 
в работе \cite{...} вводится предположение, что зависимые переменные $y_i$ измеряются
неточно, и каждая переменная $y_i$ имеет свою собственную погрешность измерения
$\sigma_{y_i}$. Затем в работе показывается, что обычный функционал суммы квадратов
регрессионных остатков, где каждый остаток нормирован на соответствующую величину
$\sigma_{y_i}^2$, корректен и статистически состоятелен.

В настоящей работе вводятся дополнительные предположения о том, что независимые
переменные также измеряются неточно, и каждая переменная имеет свою собственную
погрешность измерения. Предлагается функционал качества и модифцированный алгоритм
Левенберга-Марквардта, позволяющий найти оптимальные параметры согласно этому
функционалу качества и опирающийся на классический алгоритм
Левенберга-Марквардта (в дальнейшем будем называть их мАЛМ и АЛМ соответственно).
Доказывается сходимость модифицированного алгоритма и приводятся
результаты на экспериментальных данных по измерению насыщения лазерного излучателя.

\section{Постановка задачи}

Дана обучающая выборка $D = \{ \mathbf{x}_i, y_i \} | i \in \{ 1, \dots, \ell \}, x_i \in \mathbb{R}^m, y_i \in \mathbb{R}$.
Для каждой зависимой переменной переменной $y_i$ известно
стандартное отклонение ошибки ее измерения $\sigma_{y_i}$, а для соответствующего
вектора независимых переменных $\mathbf{x}_i$ аналогично известны стандартные
отклонения его компонент $\sigma_{x_{ij}} | j \in \{ 1, \dots, m \}$.
Пусть, кроме того, дана некоторая параметрическая регрессионная модель
$y = f (\mathbf{x}, \bomega)$.

Для удобства обозначим вектор, составленный из ошибок измерений зависимых переменных
$\sigma_{y_i}$ как $\boldsymbol{\sigma}_y$:
\[
  \boldsymbol{\sigma}_y = \{ \sigma_{y_1}, \dots, \sigma_{y_{\ell}} \}.
\]

Аналогично обозначим матрицу, составленную из ошибок измерений компонент
независимых переменных $\sigma_{x_{ij}}$ как $\Sigma_x$:
\[
  \Sigma_x = \| \sigma_{x_{ij}} \| | i \in \{ 1, \dots, \ell \}, j \in \{ 1, \dots, m \}.
\]

Требуется построить функционал ошибки $S(\bomega)$ вектора параметров
$\bomega$ модели $f$, учитывающий ошибки измерений $\sigma_{y_i}$ и
$\sigma_{x_{ij}}$:

\begin{equation}
  S(\bomega) = S(\bomega, \boldsymbol{\sigma}_y, \Sigma_x),
\end{equation}

и, кроме того, найти вектор параметров $\omega$, минимизирующий функционал
$S$:
\begin{equation}
  \hat{\bomega} = \mathop{\arg \min}\limits_{\bomega} S(\bomega)
\end{equation}

\section{Модифицированный функционал качества}

Вводятся следующие физические соображения:
\begin{enumerate}
  \item Чем больше погрешность определения зависимой переменной, тем меньше соответствующий
	регрессионный остаток должен учитываться при оптимизации параметров модели.
  \item Чем больше погрешность определения некоторой независимой переменной, тем,
	аналогично, меньше должен учитываться соответствующий регрессионный остаток.
  \item При прочих равных сильнее должны учитываться те точки, в которых производная
	регрессионной модели $\frac{\partial f}{\partial x_j}$ по соответствующей компоненте
	$x_j$ меньше.
\end{enumerate}

Рассмотрим случай $x \in \mathbb{R}$, то есть, независимая переменная всего одна.
Пользуясь вышеупомянутыми соображениями (в частности, первым и вторым), введем
следующее определение расстояния $\rho(x, i)$ от точки $(x_i, y_i)$ из обучающей выборки до
некоторой точки $(x, f(x, \bomega))$ на кривой, описываемой регрессионной моделью:

\begin{equation}
  \rho(x, i) = \frac{(x_i - x)^2}{\sigma_{x_i}^2} + \frac{(y_i - y)^2}{\sigma_{y_i}^2},
  \label{eq:dist0}
\end{equation}
где $y = f(x, \bomega)$.

Линеаризуем $f(x, \bomega)$ в окрестности точки $x_i$:
\begin{equation}
  f(x, \bomega) = f(x_i, \bomega) + (x - x_i) \frac{\partial f}{\partial x}(x_i, \bomega).
  \label{eq:f_linear}
\end{equation}
Введем для удобства обозначение $k = \frac{\partial f}{\partial x}(x_i, \bomega)$.

Тогда расстояние \eqref{eq:dist0} можно выразить через линеаризованную функцию
\eqref{eq:f_linear} следующим образом:
\begin{equation}
  \rho(x, i) = \frac{(x_i - x)^2}{\sigma_{x_i}^2} + \frac{(y_i - f(x_i, \bomega) - k (x - x_i))^2}{\sigma_{y_i}^2}.
  \label{eq:dist_linear}
\end{equation}

Минимизируем это расстояние \eqref{eq:dist_linear} по $x$, для этого возьмем
соответствующую производную и приравняем ее нулю:
\[
  2\frac{x_i - x}{\sigma_{x_i}^2} + 2k\frac{y_i - f(x_i, \bomega) - k (x - x_i)}{\sigma_{y_i}^2} = 0.
\]
Отсюда путем несложных преобразований можно получить выражение на $x - x_i$:
\[
  x - x_i = \frac{\sigma^2_{x_i} k (y_i - f(x_i, \bomega))}{\sigma^2_{y_i} + k^2 \sigma^2_{x_i}}
\]
Подставив это выражение в \eqref{eq:dist_linear}, можно получить выражение
для расстояния:
\[
  \rho(x, i) = \frac{(y_i - f(x_i, \bomega))^2}{\sigma^2_{y_i} + k^2 \sigma^2_{x_i}}.
\]

Аналогичным образом можно получить выражение для случая, когда $\mathbf{x}$~---
вектор $m$-мерном пространстве $\mathbb{R}^m$:
\[
  \rho(\mathbf{x}, i) = \frac{(y_i - f(\mathbf{x}_i, \bomega))^2}{\sigma_{y_i}^2 + \sum_{j = 1}^m (\frac{\partial f}{\partial x_j}(\mathbf{x}_i, \bomega))^2 \sigma^2_{x_{ij}}}.
\]

Таким образом, функционал, минимизирующий сумму введеных согласно \eqref{eq:dist0}
расстояний, выглядит следующим образом:
\begin{equation}
  S(\bomega) = \sum_{i = 1}^\ell \frac{(y_i - f(\mathbf{x}_i, \bomega))^2}{\sigma_{y_i}^2 + \sum_{j = 1}^m (\frac{\partial f}{\partial x_j}(\mathbf{x}_i, \bomega))^2 \sigma^2_{x_{ij}}}.
  \label{eq:s}
\end{equation}

Отметим, что этот функционал соответствует классической сумме квадратов регрессионных
остатков при условии нормировки квадрата каждого остатка на на сумму квадратов погрешности
определения зависимой величины $\sigma_{y_i}$ и произведения частной производной
регрессионной модели по $j$-ой компоненте вектора независимых величин на погрешность
определения соответствующей компоненты $\sigma_{x_{ij}}$.

Кроме того, если все независимые переменные измерены точно, то есть,
$\forall i, j : \sigma_{x_ij} = 0$, то предложенный функционал переходит в предложенный
в \cite{...}. Если же, кроме того, все зависимые переменные имеют одну и ту же погрешность,
то предложенный функционал переходит в известную сумму квадратов регрессионных остатков
(с точностью до некоторого множителя $\sigma_y$).


\FloatBarrier

\bibliographystyle{babunsrt-lf}
%\bibliographystyle{babunsrt}
%\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document}
