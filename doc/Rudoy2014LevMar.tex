\documentclass[tikz,11pt,a4paper]{article}
\usepackage{a4wide}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage{graphics,graphicx,epsfig}
\usepackage{amssymb,amsfonts,amsthm,amsmath,mathtext,cite,enumerate,float}
\usepackage[english,russian]{babel}
\usepackage[all]{xy}
\usepackage{morefloats}
\usepackage{pgf}
\usepackage[debug,outputdir={docgraphs/}]{dot2texi}
\usepackage{tikz}
\usepackage{scalefnt}
\usepackage{listings}
\usepackage{float}
\usepackage{verbatim}
\usepackage{placeins}
\usepackage{url}
\usepackage{babelbib}
\usepackage{pbox}
\usepackage{grffile}
\usepackage{color}
\usepackage{xfrac}
\usepackage{comment}
\usepackage{rotating}
\usepackage{slashbox}
\usepackage{caption}
\usepackage{subcaption}
\usetikzlibrary{arrows,shapes,intersections,decorations.markings,calc}

\makeatletter
\def\@settitle{\begin{center}%
    \baselineskip14\p@\relax
    \bfseries
    \@title
  \end{center}%
}

\makeatother

\newcommand{\bomega}{\boldsymbol{\omega}}

\begin{document}

\begin{center}
  МОДИФИКАЦИЯ ФУНКЦИОНАЛА ОШИБКИ В ЗАДАЧАХ НЕЛИНЕЙНОЙ РЕГРЕССИИ ДЛЯ УЧЕТА ПОГРЕШНОСТИ В ИЗМЕРЯЕМЫХ ДАННЫХ

  \bigskip
  Г.\,И.~Рудой
\end{center}

\begin{abstract}
  Рассматривается случай существенно нелинейной регрессионной зависимости
  в физическом эксперименте с гетероскедастичными погрешностями измерения
  как зависимых, так и независимых переменных.
  Предлагается модифицированный функционал среднеквадратичной ошибки,
  учитывающий ошибки в независимых переменных и различные распределения
  ошибок в разных точках. Рассматривается сходимость минимизирующего этот функционал
  вектора параметров к вектору параметров, оптимальному для классического
  функционала среднеквадратичной ошибки.
  Приводятся результаты численного моделирования на данных, полученных в ходе
  эксперимента по измерению зависимости мощности лазера от прозрачности
  резонатора.

  \textbf{Ключевые слова}: \emph{гетероскедастичные ошибки,
  ошибки измерения независимых переменных, символьная регрессия, нелинейные модели.}
\end{abstract}

\section{Введение}

В ряде экспериментальных приложений возникает задача нахождения оптимальных
коэффициентов $\boldsymbol{\omega}$ некоторой регрессионной модели $f$, представленной в
виде аналитической формулы, по набору экспериментальных данных. Для этого
в предположении о нормальном распределении регрессионных остатков
строится функционал, являющийся суммой квадратов отклонений экспериментальных
точек $y_i$ от регрессионной кривой: $\sum_i (y_i - f(x_i, \boldsymbol{\omega}))^2$.

Однако, данный функционал построен и статистически обоснован в предположении
о точно измеренных независимых переменных и гомоскедастичности ошибок измерения
зависимой переменной. Иными словами, рассматриваются только ошибки измерения
зависимой переменной, для которых дисперсия соответствующего распределения
принимается одинаковой.

В большинстве естественнонаучных приложений это предположение не выполняется,
особенно при измерении некоторой зависимости в достаточно широких диапазонах.
Например, в задаче нахождения дисперсионной зависимости прозрачного
полимера (то есть, зависимости коэффициента преломления $n$ от длины волны
$\lambda$) \cite{Rudoy15MonteCarlo} погрешности измерения
различных физических параметров, вообще говоря, различны. Так, например, если
для измерения длины волны $\lambda$ используется дифракционная решетка, то постоянной
является относительная погрешность определения длины волны
$\frac{\sigma_{\lambda_i}}{\lambda_i} \approx \text{const}$, и, следовательно,
погрешность определения длины волны зависит от самой длины волны.

Таким образом, возникает задача поиска оптимальных коэффициентов регрессионной
формулы с учетом отличающихся погрешностей измерения в различных экспериментальных точках.
Для некоторых частных случаев эта задача уже была решена: например, 
в работе \cite{jukic2013nonlinear} вводится предположение, что зависимые переменные $y_i$ измеряются
неточно, и каждая переменная $y_i$ имеет свою собственную погрешность измерения
$\sigma_{y_i}$.

В настоящей работе рассмотрена более общая ситуация, в которой независимые
переменные также определяются неточно в процессе эксперимента, и каждая переменная
имеет свою собственную погрешность измерения. При этом рассматривается случай
нелинейной регрессионной зависимости, в отличие от, например,
\cite{kiryati2000heteroscedastic}, изучающей линейную регрессионную модель.

В настоящей работе предлагается модифицированный функционал качества, учитывающий погрешности как
зависимых, так и независимых переменных в виде, достаточном для большинства
практических приложений. Рассматривается модификация имеющихся методов
оптимизации, применяемых в подобных задачах, на примере широко применяемого
алгоритма Левенберга-Марквардта\cite{Marquardt1963Algorithm} (классический
и модифицированный алгоритмы будут в дальнейшем называться АЛМ и мАЛМ
соответственно).
Показывается корректность модифицированного алгоритма и приводятся
результаты на экспериментальных данных по измерению параметров усиливающей
среды газового излучателя.

\section{Постановка задачи}

Дана обучающая выборка $D$:
\begin{equation}
  D = \{ \mathbf{x}_i, y_i \} | i \in \{ 1, \dots, \ell \}, \mathbf{x}_i \in \mathbb{R}^m, y_i \in \mathbb{R}.
  \label{eq:d}
\end{equation}
Для каждой зависимой переменной переменной $y_i$ известно
стандартное отклонение ошибки ее измерения $\sigma_{y_i}$, а для соответствующего
вектора независимых переменных $\mathbf{x}_i$ аналогично известны стандартные
отклонения его компонент $\sigma_{x_{ij}} | j \in \{ 1, \dots, m \}$.
Пусть, кроме того, дана некоторая регрессионная модель
$y = f (\mathbf{x}, \bomega)$, параметризованная вектором $\bomega$.

Для удобства введем вектор, составленный из ошибок измерений зависимых переменных
$\sigma_{y_i}$:
\[
  \boldsymbol{\sigma}_y = \{ \sigma_{y_1}, \dots, \sigma_{y_{\ell}} \}.
\]

Аналогично введем матрицу, составленную из ошибок измерений независимых переменных
$\sigma_{x_{ij}}$:
\[
  \Sigma_x = \| \sigma_{x_{ij}} \| | i \in \{ 1, \dots, \ell \}, j \in \{ 1, \dots, m \}.
\]

Требуется построить функционал ошибки $S(\bomega)$ вектора параметров
$\bomega$ модели $f$, учитывающий ошибки измерений $\boldsymbol{\sigma}_y$ и
$\Sigma_x$:
\begin{equation}
  S(\bomega) = S(\bomega, \boldsymbol{\sigma}_y, \Sigma_x),
  \label{eq:s_modified}
\end{equation}
и, кроме того, найти вектор параметров $\omega$, минимизирующий функционал
\eqref{eq:s_modified}:
\begin{equation}
  \hat{\bomega} = \mathop{\arg \min}\limits_{\bomega} S(\bomega).
\end{equation}

\section{Модифицированный функционал качества}

Воспользуемся следующим естественным качественным физическим соображением:
чем больше погрешность определения зависимой и независимых переменных
для некоторой экспериментальной точки, тем меньше соответствующий
регрессионный остаток должен учитываться при оптимизации параметров модели.
Кроме того, эта зависимость линейна: если, например, $\sigma_{x_{i1}} = k \sigma_{x_{i2}}$,
т. е. если погрешность измерения первой компоненты признакового описания в $k$
раз больше погрешности второй компоненты, то расстояние от экспериментально
измеренной точки $(\mathbf{x}_i, y_i)$ до регрессионной зависимости по оси,
соответствующей первой компоненте, имеет в $k$ раз меньший вес, чем то же
расстояние по оси, соответствующей второй компоненте. Можно заметить, что
это эквивалентно некоторому локальному растяжению пространства в окрестности
точки $(\mathbf{x}_i, y_i)$. Кроме того, с физической точки зрения складывать
можно только величины, имеющие одинаковую размерность, либо безразмерные
величины, поэтому необходима некоторая нормировка невязок по каждому из
измерений.

Рассмотрим для простоты изложения случай одной независимой переменной:
$x \in \mathbb{R}$. С учетом приведенных выше соображений введем
следующее определение расстояния $\rho(x, i)$
от точки $(x_i, y_i)$ из обучающей выборки до некоторой точки
$(x, f(x, \bomega))$ на кривой, описываемой регрессионной моделью:
\begin{equation}
  \rho^2(x, i) = \frac{(x_i - x)^2}{\sigma_{x_i}^2} + \frac{(y_i - y)^2}{\sigma_{y_i}^2},
  \label{eq:dist0}
\end{equation}
где $y = f(x, \bomega)$.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}[
      scale = 1.5,
      point/.style = { draw, black, circle, fill, scale = 0.2 },
      classical/.style = { magenta },
      linearized/.style = { blue },
      ideal/.style = { orange },
      tangent/.style = {
        decoration = {
        markings,
        mark =
          at position #1
          with
          {
            \coordinate (tangent point-\pgfkeysvalueof{/pgf/decoration/mark info/sequence number}) at (0pt, 0pt);
            \coordinate (tangent unit vector-\pgfkeysvalueof{/pgf/decoration/mark info/sequence number}) at (1, 0pt);
            \coordinate (tangent orthogonal unit vector-\pgfkeysvalueof{/pgf/decoration/mark info/sequence number}) at (0pt, 1);
          }
        },
        postaction=decorate
      },
      use tangent/.style = {
        shift = (tangent point-#1),
        x = (tangent unit vector-#1),
        y = (tangent orthogonal unit vector-#1)
      },
      use tangent/.default = 1
    ]
    \coordinate (O) at (0,0);
    \draw[->, name path = Ox] (-0.3,0) -- (8,0) coordinate[label = { below:$x$ }] (xmax);
    \draw[->, name path = Oy] (0,-0.3) -- (0,6) coordinate[label = { right:$y$ }] (ymax);

    \draw[thick, red, tangent = 0.57, name path = f_x] plot[smooth] coordinates {(1, 4.875) (2, 4.75) (3, 4.5) (4, 4) (5, 3) (6, 1) };
    \node at (2, 5) {$f(x, \bomega)$};

    \draw[ideal, dashed, use tangent = 1] (-2, 0) -- (2, 0);
    \draw[ideal, use tangent = 1]
      (0, 0) node[point] (ideal_cp) {} --
      (0, 1.5) node[point, label = { [black]right:$(x_i, y_i)$ }] (datapt) {}
          node[pos = 0.6, label = { [black]below:$\tilde{\rho}$ }] {};

    \path[name path = vert_ideal_cp] (ideal_cp) -- ++ (270:5);
    \path[name path = hor_ideal_cp] (ideal_cp) -- ++ (180:5);
    \draw[ideal, dotted, name intersections = {of=vert_ideal_cp and Ox}]
      (ideal_cp) -- (intersection-1) node[point, label = { below left:$\tilde{x}$}] {};
    \draw[ideal, dotted, name intersections = {of=hor_ideal_cp and Oy}]
      (ideal_cp) -- (intersection-1) node[point, label = { left:$f(\tilde{x}, \bomega)$}] {};

    \path[name path = vertical_dp] (datapt) -- ++ (270:5);
    \draw[classical, name intersections = {of=f_x and vertical_dp, by={intersect}}]
      (datapt) --
      (intersect) node[point] (classical_cp) {}
          node[pos = 0.5, label = { [black]right:$y_i - f(x_i, \bomega)$ }] {};

    \path[name path = vert_classical_cp] (classical_cp) -- ++ (270:3);
    \path[name path = hor_classical_cp] (classical_cp) -- ++ (180:6);
    \draw[classical, dotted, name intersections = {of=vert_classical_cp and Ox}]
      (classical_cp) -- (intersection-1) node[point, label = { below:$x_i$ }] {};
    \draw[classical, dotted, name intersections = {of=hor_classical_cp and Oy}]
      (classical_cp) -- (intersection-1) node[point, label = { left:$f(x_i, \bomega)$ }] {};

    \path[name path = almost_vertical_dp] (datapt) -- ++ (271.5:5);
    \path[name intersections = {of=f_x and almost_vertical_dp}]
      (datapt) --
      (intersection-1) node[point, transparent] (almost_classical_cp) {};

    \draw[linearized, dashed, shorten >= -3cm, shorten <= -3cm] (classical_cp) -- (almost_classical_cp);

    \node[linearized, point] (linearized_orto) at ($(classical_cp)!(datapt)!(almost_classical_cp)$) {};
    \draw[linearized] (linearized_orto) -- (datapt)
          node[pos = 0.4, label = { [black]above:$\rho$ }] {};

    \path[name path = vert_linearized] (linearized_orto) -- ++ (270:5);
    \path[name path = hor_linearized] (linearized_orto) -- ++ (180:5);
    \draw[linearized, dotted, name intersections = {of=vert_linearized and Ox}]
      (linearized_orto) -- (intersection-1) node[point, label = { below right:$\hat{x}$ }] {};
    \draw[linearized, dotted, name intersections = {of=hor_linearized and Oy}]
      (linearized_orto) -- (intersection-1) node[point, label = { left:$\mathbb{L}_i[f](\hat{x}, \bomega)$ }] {};
  \end{tikzpicture}
  \caption{Различные способы введения расстояния от точки до прямой: $\tilde{\rho}$~---
    истинное расстояние как минимум расстояния от точки $(x_i, y_i)$ до какой-либо
    точки на прямой, $y_i - f(x_i, \boldsymbol{\omega})$~--- расстояние в классическом
    функционале среднеквадратичной ошибки в предположении об отсутствии ошибок измерения
    независимой переменной, $\rho$~--- предлагаемое нами расстояние.}
  \label{fig:distances}
\end{figure}

Непосредственное точное определение расстояния от экспериментальной
точки до регрессионной кривой представляется отдельной
сложной вычислительной задачей, однако, в подавляющем большинстве
практических приложений регрессионные зависимости достаточно
гладкие, а погрешности измерения достаточно малы. Пользуясь этим,
линеаризуем $f(x, \bomega)$ в окрестности точки $(x_i, f(x_i, \bomega))$:
\begin{equation}
  f(x, \bomega) \approx \mathbb{L}_{i}[f](x, \bomega) = f(x_i, \bomega) + (x - x_i) \frac{\partial f}{\partial x}(x_i, \bomega),
  \label{eq:f_linear}
\end{equation}
где $\mathbb{L}_i$ обозначает оператор линеаризации в окрестности точки
$(x_i, f(x_i, \bomega))$.

Тогда расстояние \eqref{eq:dist0} можно выразить через линеаризованную функцию
\eqref{eq:f_linear} следующим образом:
\begin{equation}
  \rho^2(x, i) = \frac{(x_i - x)^2}{\sigma_{x_i}^2} + \frac{(y_i - f(x_i, \bomega) - \frac{\partial f}{\partial x}(x_i, \bomega) (x - x_i))^2}{\sigma_{y_i}^2}.
  \label{eq:dist_linear}
\end{equation}

Минимизируя это выражение, находим выражение для расстояния от
точки $(x_i, y_i)$ из обучающей выборки до линеаризованной в ее
окрестности регрессионной зависимости:
\begin{equation}
  \rho^2(x, i) = \frac{(y_i - f(x_i, \bomega))^2}{\sigma^2_{y_i} + \frac{\partial f}{\partial x}(x_i, \bomega)^2 \sigma^2_{x_i}}.
  \label{eq:rho_univar}
\end{equation}

Аналогично можно получить выражение для случая, когда объекты в обучающей выборке
имеют $m$ независимых переменных ($\mathbf{x} \in \mathbb{R}^m$):
\[
  \rho^2(\mathbf{x}, i) = \frac{(y_i - f(\mathbf{x}_i, \bomega))^2}{\sigma_{y_i}^2 + \sum_{j = 1}^m (\frac{\partial f}{\partial x_j}(\mathbf{x}_i, \bomega))^2 \sigma^2_{x_{ij}}}.
\]

Таким образом, предлагаемый нами функционал, минимизирующий сумму введеных согласно \eqref{eq:dist0}
расстояний, для достаточно гладких функций выглядит следующим образом:
\begin{equation}
  S(\bomega) = \sum_{i = 1}^\ell \frac{(y_i - f(\mathbf{x}_i, \bomega))^2}{\sigma_{y_i}^2 + \sum_{j = 1}^m (\frac{\partial f}{\partial x_j}(\mathbf{x}_i, \bomega))^2 \sigma^2_{x_{ij}}}.
  \label{eq:s}
\end{equation}

Отметим следующее:
\begin{itemize}
  \item Функционал \eqref{eq:s} соответствует классической сумме квадратов регрессионных
	остатков при условии нормировки квадрата каждого остатка на сумму квадратов погрешности
	определения зависимой величины $\sigma_{y_i}$ и произведения частной производной
	регрессионной модели по $j$-ой компоненте вектора независимых величин на погрешность
	определения соответствующей компоненты $\sigma_{x_{ij}}$.

  \item При прочих равных условиях в выражении для расстояния \eqref{eq:rho_univar} и,
	соответственно, в функционале \eqref{eq:s} с большим весом учитываются те точки, в которых
	производная регрессионной модели $\frac{\partial f}{\partial x_j}$ по соответствующей
	компоненте $x_j$ больше, что соответствует соображениям здравого смысла: чем меньше наклон
	регрессионной зависимости в окрестности данной точки, тем меньше влияние неточного
	измерения соответствующей независимой переменной на значение регрессионной зависимости
	в этой точке.

  \item Если все независимые переменные измерены точно, то есть,
	$\forall i, j : \sigma_{x_{ij}} = 0$, то предложенный функционал переходит в рассмотренный
	в \cite{jukic2013nonlinear}. Если же, кроме того, все зависимые переменные имеют одну и ту же погрешность,
	то предложенный функционал переходит в известную сумму квадратов регрессионных остатков
	с точностью до некоторого множителя $\sigma_y$, не влияющего на точки минимума функционала
	среднеквадратичной ошибки.
\end{itemize}

\section{Модифицированный алгоритм Левенберга-Марквардта}

Для численной оптимизации функционала \eqref{eq:s} представим его в виде
суммы квадратов регрессионных остатков путем следующего переобозначения переменных.
Вместо выборки \eqref{eq:d}
рассмотрим выборку
\[
  \tilde{D} = \{ \tilde{\mathbf{x}}_i, \tilde{y}_i \} | i \in \{ 1, \dots, \ell \}, \tilde{\mathbf{x}}_i \in \mathbb{R}^{m + 1}, y_i \in \mathbb{R},
\]
где $\tilde{y}_i \equiv 0$, а
$\tilde{\mathbf{x}}_i = \{ \mathbf{x}_i, y_i \}$~--- исходный вектор $\mathbf{x}_i$
с дополнительно приписанным к нему значением $y_i$. Кроме того, примем
\[
  \tilde{f}(\tilde{\mathbf{x}}_i, \bomega) = \frac{f(\mathbf{x}_i, \bomega) - y_i}{\sqrt{\sigma_{y_i}^2 + \sum_{j = 1}^m (\frac{\partial f}{\partial x_j}(\mathbf{x}_i, \bomega))^2 \sigma^2_{x_{ij}}}}.
\]
Тогда минимизация функционала \eqref{eq:s} возможна известными методами оптимизации, так
как легко видеть, что \eqref{eq:s} в этом случае эквивалентен
\[
  S(\bomega) = \sum_{i = 1}^\ell (\tilde{y}_i - \tilde{f}(\tilde{\mathbf{x}}_i, \bomega))^2.
\]

При невозможности выполнить аналитическое дифференцирование функции $f$ предлагается
следующий итеративный алгоритм, предназначенный для использования с уже имеющимися
реализациями методов оптимизации. Предполагается, что соответствующая реализация
<<принимает на вход>> массив значений $y_i$,
функцию вычисления значения $f$ в точках $\mathbf{x}_i$ с вектором параметров $\bomega$,
и критерий останова в виде числа итераций. Пример такой реализации представлен в \cite{dlib09}.

Алгоритм выглядит следующим образом:
\begin{enumerate}
  \item Выбирается некоторое начальное приближение вектора параметров $\bomega$.
  \item Для каждой пары $(\mathbf{x}_i, y_i)$ из обучающей выборки рассчитывается значение
	частной производной $\frac{\partial f}{\partial x}$ в точке $(\mathbf{x}_i, \bomega)$.
  \item Каждое значение зависимой переменной $y_i$ и значение функции $f(\mathbf{x}_i, \bomega)$
	нормируется на соответствующую величину
	\[
	  \sigma_{y_i}^2 + \sum_{j = 1}^m (\frac{\partial f}{\partial x_j}(\mathbf{x}_i, \bomega))^2 \sigma^2_{x_{ij}},
	\]
    где вместо самой частной производной может использоваться ее некоторое численное
    приближение.
  \item Выполняется итерация классического алгоритма оптимизации для таким образом
	модифицированных значений функции $f$ и зависимых переменных $y_i$, таким образом
	получается новое значение вектора $\bomega$.
  \item Если критерий останова не достигнут, алгоритм продолжает выполнение с пункта 2.
\end{enumerate}

Отметим следующее:
\begin{itemize}
  \item Критерием останова могут служить обычные критерии вроде достижения некоторого
    числа итераций, нормы изменения вектора $\bomega$, и т.~п.
  \item Если известно, что производная $\frac{\partial f}{\partial x}$ является достаточно гладкой
	в окрестности $(\mathbf{x}_i, \bomega) \mid i \in \{ 1, \dots, \ell \}$, на шаге
	4 алгоритма представляется разумным выполнить сразу несколько итераций
	классического алгоритма во избежание потенциально ресурсоемкого пересчета производных и
	перенормировки значений $y_i$ и $f$.
\end{itemize}

\section{Вычислительный эксперимент}

В вычислительном эксперименте рассматриваются данные, полученные в ходе измерения
зависимости интенсивности излучения $I$ лазера от прозрачности его резонатора.
Изучался лазер высокого давления ($\approx 3\ \text{атм}\ He, \approx 60\ \text{Торр}\ Ne, \approx 20\ \text{Торр}\ Ar$) на
$3p-3s$ переходах неона (основной переход~--- 585 нм), возбуждаемый электронным пучком.

Пусть насыщающая переход интенсивность излучения~--- $I_s$, наблюдаемая интенсивность~---
$I_l$. В таком случае для безразмерной величины $y = \frac{I_l}{I_s}$ с учетом однородного
уширения линии усиления при высоком давлении газа и хорошей однородности возбуждения,
обеспечиваемой электронным пучком, можно получить нелинейное уравнение \cite{champagne1982transient}:
\begin{equation}
  \alpha_0 L - \frac{1}{2} \ln R_0 = g_0 L \frac{1 + \sqrt{R_0}}{1 - \sqrt{R_0}} \frac{1}{y} \ln \Big( 1 + \frac{y \frac{1 - \sqrt{R_0}}{1 + \sqrt{R_0}}}{1 + y \frac{2 \sqrt{R_0}}{1 - R_0}} \Big),
  \label{eq:y_exact}
\end{equation}
где $\alpha_0$~--- распределенные потери (например, на рассеяние света),
$g_0$~--- коэффициент усиления слабого сигнала, $R_0$~--- коэффициент отражения выходного
зеркала лазера. Однородность накачки означает, что $g_0$ и $\alpha_0$ одинаковы по
всему объему с хорошей точностью.

Значение $R_0$ является независимой переменной, изменяемой экспериментаторами, и в данном
разделе также обозначается как $x$ сообразно остальной части работы.

Для достаточно больших $R_0$, близких к единице, можно заменить
$2 \sqrt{R_0} \approx 1 + R_0$ в \eqref{eq:y_exact}, получив выражение
\begin{equation}
  y(R_0) = \gamma \frac{1 - R_0}{1 + R_0} \Big(\frac{g_0}{\alpha_0 - \frac{1}{2} L \ln R_0} - 1\Big),
\end{equation}
где $\gamma$~--- нормировочный коэффициент.

Длина активной среды $L$~--- 150 см, точность определения мощности лазера $y$ имеет
относительную погрешность в $2\%$, точность определения прозрачности $R_0$ имеет
абсолютную погрешность и составляет $0.01$ при $R_0 \geq 0.6$ и $0.1$ иначе.

В ходе физических измерений получены значения $y(R_0)$, приведенные в таблице \ref{tabl:measurements}.

\begin{table}[h]
  \caption{Экспериментальные значения $y(R_0)$.}
  \centering
  \begin{tabular}{| r | r | r | r | r | r | r | r |}
	\hline
	$R_0$	&	0.48	&	0.56	&	0.65	&	0.73	&	0.80	&	0.87	&	0.94	\\ \hline
	$y$		&	3.25	&	10.2	&	16.5	&	20.5	&	22.5	&	23.2	&	18.2	\\ \hline
  \end{tabular}
  \label{tabl:measurements}
\end{table}

Таким образом, решается задача минимизации функционала \eqref{eq:s} при
\[
  \boldsymbol{\omega} = (\omega_1, \omega_2, \omega_3) = (\gamma, \alpha_0, g_0),
\]
\[
  f(x, \boldsymbol{\omega}) = y(R_0, \gamma, \alpha_0, g_0),
\]
\[
  \begin{array}{ll}
	\sigma_{y_i} &= 0.02 y_i,\\
	\sigma_{x_i} &= \left\{
	  \begin{array}{lr}
		0.01 & \mid x_i \geq 0.6, \\
		0.1  & \mid x_i < 0.6.
	  \end{array}
	\right.
\end{array}
\]

\subsection{Оптимальные параметры модели}
Кроме предложенного в настоящей работе функционала \eqref{eq:s} рассмотрен
также и классический функционал среднеквадратичной ошибки:
\begin{equation}
  S = \sum_{i = 1}^\ell (y_i - f(x_i, \boldsymbol{\omega}))^2,
  \label{eq:s_classic}
\end{equation}

В таблице \ref{tabl:res_coeffs} приведены значения параметров $\bomega$ и $\bomega^0$,
минимизирующие \eqref{eq:s} и \eqref{eq:s_classic} соответственно, а также относительные
разности их компонент. Кроме того, приведены значения функционалов \eqref{eq:s} и
\eqref{eq:s_classic} для обоих векторов параметров.

\begin{table}[h]
  \caption{Оптимальные значения параметров модели.}
  \centering
  \begin{tabular}{| r | r | r | r | r | r | r | r | r |}
	\hline
													& $g_0$					& $\alpha_0$			& $\gamma$				& Значение \eqref{eq:s} & Значение \eqref{eq:s_classic}	\\ \hline
	$\bomega$										& $2.932 \cdot 10^{-3}$	& $2.219 \cdot 10^{-4}$	& $99.2$				& $0.539$				& $0.282$						\\ \hline
	$\bomega^0$										& $2.917 \cdot 10^{-3}$	& $2.129 \cdot 10^{-4}$	& $101.5$				& $0.642$				& $0.183$						\\ \hline
	$\sfrac{|\omega_i - \omega^0_i|}{\omega^0_i}$	& $5.2 \cdot 10^{-3}$	& $4.2 \cdot 10^{-2}$	& $2.4 \cdot 10^{-2}$	& $0.16$				& $0.54$						\\ \hline
  \end{tabular}
  \label{tabl:res_coeffs}
\end{table}

\subsection{Сходимость оптимальных параметров}
Численно исследована зависимость сходимости параметров $\bomega$ к параметрам $\bomega^0$,
получаемым минимизацией функционалов \eqref{eq:s} и \eqref{eq:s_classic} соответственно, от
погрешности $\mathbf{\sigma}_y$ измерения зависимой переменной $y$.

Разумно ожидать, что при увеличении погрешности измерения величины $y$ при
фиксированной погрешности измерения $R_0$ оптимальный вектор $\boldsymbol{\omega}$
будет приближаться к $\boldsymbol{\omega}^0$, так как тем более незначителен
вклад ошибки измерения независимой переменной.

Рассматриваются два случая:
\begin{enumerate}
  \item Погрешность $i$-го измерения $y_i$ задается как $\sigma_{y_i} = 0.02ky_i$, т. е.
	погрешность зависит от значения самого $y_i$.
  \item Погрешность $i$-го измерения $y_i$ задается как $\sigma_{y_i} = 0.02ky_{\max}$,
	т. е. погрешность от значения конкретного $y_i$ не зависит. Заметим, что выбор
	конкретного значения $y$, определяющего погрешность, является в данном случае
	достаточно произвольным и соответствует умножению всех погрешностей на некоторую константу.
\end{enumerate}

В первом случае ошибки измерения $y$ распределены неодинаково, следовательно, применение
стандартного метода наименьших квадратов не обосновано. В то же время во втором случае
ошибки принадлежат одному и тому же распределению, и, кроме того, независимы, поэтому
в данном случае МНК-оценка применима (с точностью до ошибки измерения независимой
переменной).

Для обоих случаев подробно рассматривалась область $k \in [1; 100]$, значение $k$
изменялось с шагом 0.01. Отметим, что уже при $k = 50$ характерная погрешность
измерения величины $y$ сопоставима с самой величиной $y$, а при больших значениях
$k$ превышает ее.

Результаты приведены на рис. \ref{fig:conv_varY_100} и \ref{fig:conv_fixedY_100}
соответственно. На графиках отображены отношения компонент вектора $\boldsymbol{\omega}$ к
соответствующим коэффицентам $\boldsymbol{\omega}^0$ в зависимости от значения $k$.

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{figs/levmar/convergence/varY/convergence_100.txt.pdf}
	\caption{$\sigma_{y_i} = 0.02ky_i$}
	\label{fig:conv_varY_100}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{figs/levmar/convergence/fixedXY_0.1/convergence_100.txt.pdf}
	\caption{$\sigma_{y_i} = 0.02ky_{\max}$}
	\label{fig:conv_fixedY_100}
  \end{subfigure}
  \caption{Зависимость оптимальных параметров от $k \in [1; 100]$.}
  \label{fig:conv_varY}
\end{figure}

Видно, что в случае фиксированной погрешности для всех $y_i$ значения $\boldsymbol{\omega}$
действительно стремятся к $\boldsymbol{\omega}^0$ для разумных значений $k$, а в случае
гетероскедастичности ошибки такой зависимости не наблюдается, хотя значения $\boldsymbol{\omega}$
и оказываются достаточно близки к $\boldsymbol{\omega}^0$.

\section{Заключение}

Предложен модифицированный функционал среднеквадратичной ошибки для существенно
нелинейных моделей, применимый в случае наличия ошибок измерения независимых
переменных и различия распределения, к которому принадлежат ошибки, в разных
точках обучающей выборки.

Показана сходимость предложенного функционала к классическому функционалу
среднеквадратичной ошибки для случая гомоскедастичности погрешностей
зависимой переменной и пренебрежимо малой погрешности измерения независимых
переменных.

\FloatBarrier

\bibliographystyle{babunsrt-lf}
%\bibliographystyle{babunsrt}
%\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document}
